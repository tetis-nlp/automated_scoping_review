{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## 1. Load DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/quantitative_value_with_context_CC.xlsx'\n",
    "\n",
    "# Read the \"missing value\" worksheet\n",
    "missing_value_df = pd.read_excel(file_path, sheet_name='missing value')\n",
    "\n",
    "# Read the 'quantitative_value_with context' worksheet\n",
    "quantitative_value_df = pd.read_excel(file_path, sheet_name='quantitative_value_with context')\n",
    "# quantitative_value_df = quantitative_value_df.rename(columns={'Relevant context for MOOD data extraction': 'context'})\n",
    "\n",
    "missing_value_df['source'] = 'missing_value'\n",
    "quantitative_value_df['source'] = 'quantitative_value_with_context'\n",
    "\n",
    "# drop 3 lines corresponding to a Claudia's comment\n",
    "\"\"\"\n",
    "Missing values not found by the script. Articles: MB7, MB8, CC6-15-17-23-32\n",
    "\"\"\"\n",
    "missing_value_df = missing_value_df.drop(index=range(53, 56))\n",
    "\n",
    "# concatenate the 2 dataframe\n",
    "# df = pd.concat([missing_value_df, quantitative_value_df], ignore_index=True)\n",
    "df = quantitative_value_df\n",
    "\n",
    "usable_covariates = df[df[\"Relevant context for MOOD data extraction\"].str.lower().isin([\"yes\", \"Yes\"])]\n",
    "# usable_covariates = pd.concat([missing_value_df, usable_covariates])\n",
    "\n",
    "full_text_annotation = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"no\", \"No\"])]\n",
    "table_annotations = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"Table\", \"table\", \"table and caption\"])]\n",
    "figure_annotations = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"figure\", \"Figure\", \"Figure caption\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_covariates[[\"context\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = f'{full_text_annotation.iloc[0][\"context\"]} \\t covariable found: {full_text_annotation.iloc[0][\"covariate_found_in_text\"]}: {(full_text_annotation.iloc[0][\"non-standardized covariate in the context\t\"] if full_text_annotation.iloc[0][\"covariate_found_in_text\"] else \"\")}'\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_annotation[\"covariate_text\"] = full_text_annotation.apply(lambda row: row[\"non-standardized covariate in the context\"] if row[\"Relevant context for MOOD data extraction\"] else \"\", axis=1)\n",
    "full_text_annotation[\"docs\"] = full_text_annotation.apply(lambda row: f'{row[\"context\"]} \\t covariable found: {row[\"covariate_found_in_text\"]}: {row[\"covariate_text\"]}', axis=1)\n",
    "\n",
    "print(full_text_annotation[\"docs\"].head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = full_text_annotation[\"docs\"].tolist()\n",
    "print(f\"Nb of docs: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict.load_from_disk(\"./data/annotation_generated_from_xlsx/annotation.dataset\")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import unicodedata\n",
    "\n",
    "train_docs_list = docs.copy()\n",
    "test_docs_list = []\n",
    "\n",
    "for i, sentence in enumerate(dataset_dict[\"test\"][\"tokens\"]):\n",
    "    s = \" \".join(sentence)\n",
    "    print(f\"{i}: {s}\")\n",
    "    for doc in docs:\n",
    "        doc_format = unicodedata.normalize(\"NFKD\", doc.lstrip())\n",
    "        if SequenceMatcher(None, doc_format.lower(), s.lower()).ratio() > 0.7:\n",
    "            # print(f\"{s.lower()} \\n\\t {doc_format.lower()}\")\n",
    "            # print(\"\\n\")\n",
    "            # print(f\"{s.lower()[0]} | {doc_format.lower()[0]}\")\n",
    "            if doc_format.lower()[0] == s.lower()[0]:\n",
    "                print(f\"\\t found !: {doc}\")\n",
    "                train_docs_list.remove(doc)\n",
    "                test_docs_list.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_docs_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepage RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "#from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs_for_rag = text_splitter.create_documents(train_docs_list)\n",
    "\n",
    "db = FAISS.from_documents(docs_for_rag, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"OpenAI API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context as example to help to extract covariates or risk factors from the sentence only (don't extract from the context please). If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain_gpt4 = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    # | hub.pull(\"rlm/rag-prompt\")\n",
    "    | prompt_template\n",
    "    # | ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "    | ChatOpenAI(model_name=\"gpt-4\", temperature=0, openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"g10 (i.e. a 90% reduction in infectivity). These Rt values were based on the averages reported by Keeler et al. (2014) for resistance of 9 LPAIV strains in distilled, ï¬ltered and natural water (67.1, 3.1 and 30.0 days, respectively), and were also in accordance with previously published data (e.g. Stallknecht et al., 1991; Brown et al., 2009; Lebarbenchon et al., 2012 ). 2.7. The numerical model I\"\"\"\n",
    "\n",
    "rag_chain_gpt4.invoke(\"Is there any covariate (or risk factor) in this following sentence: \\n \" + sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for sentence in test_docs_list:\n",
    "    res = rag_chain_gpt4.invoke(\"Is there any covariate (or risk factor) in this following sentence: \\n \" + sentence)\n",
    "    results_list.append(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_found = 0\n",
    "for res in results_list:\n",
    "    if res.lower().startswith(\"yes\"):\n",
    "        covariate_found += 1\n",
    "print(f\"Nb of covariate found: {covariate_found} | {100*covariate_found/len(test_docs_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, r in zip(test_docs_list, results_list):\n",
    "    print(f\"{s} \\n \\t{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = hub.pull(\"rlm/rag-prompt\")\n",
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same with GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    # | hub.pull(\"rlm/rag-prompt\")\n",
    "    | prompt_template\n",
    "    | ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "    # | ChatOpenAI(model_name=\"gpt-4\", temperature=0, openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_gpt3_5 = []\n",
    "\n",
    "for sentence in test_docs_list:\n",
    "    res = rag_chain.invoke(\"Is there any covariate (or risk factor) in this following sentence: \\n \" + sentence)\n",
    "    results_list_gpt3_5.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, r in zip(test_docs_list, results_list_gpt3_5):\n",
    "    print(f\"{s} \\n \\t{r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_gpt3_5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Infer on all diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid.tei import Parser\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_files_path = \"./data/grobid/\"\n",
    "\n",
    "tei_file_example = \"./data/grobid/CC10 Aerosol Susceptibility of Influenza Virus to UV-C Light.pdf.tei.xml\"\n",
    "\n",
    "# list_of_diseases = [\"chikungunya\", \"leptospirosi\", \"influenza\" ]\n",
    "list_of_diseases = [\"influenza\" ]\n",
    "sections_to_keep = [\"abstract\", \"results\", \"discussion\"]\n",
    "chunk_size = 256\n",
    "\n",
    "list_of_base_models = [\"GPT-3.5\", \"GPT-4\"]\n",
    "# list_of_base_models = [\"GPT-4\"]\n",
    "# list_of_base_models = [\"GPT-3.5\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_relevant_sections(tei_file):\n",
    "    chunks = []\n",
    "    md = {\n",
    "        \"title\": tei_file.split(\"/\")[-1],\n",
    "        \"id\": (tei_file.split(\"/\")[-1]).split(\" \")[0],\n",
    "    }\n",
    "\n",
    "    # load PDF into python dict\n",
    "    with open(tei_file, \"rb\") as xml_file:\n",
    "        xml_content = xml_file.read()\n",
    "    parser = Parser(xml_content)\n",
    "    article = parser.parse()\n",
    "    article = json.loads(article.to_json())\n",
    "\n",
    "    # work on abstract\n",
    "    abstract = article[\"abstract\"]\n",
    "    for p, paragraph in enumerate(abstract[\"paragraphs\"]):\n",
    "        for i in range(0, len(paragraph[\"text\"]), chunk_size):\n",
    "            # print(f\"{i} : {(i)} | {i + chunk_size -1}\")\n",
    "            chunk_with_md = md.copy()\n",
    "            chunk_with_md[\"section\"] = \"abstract\"\n",
    "            chunk_with_md[\"paragraph_nb\"] = p\n",
    "            chunk_with_md[\"chunk_nb\"] = i/chunk_size\n",
    "            chunk_with_md[\"text\"] = paragraph[\"text\"][i:i+chunk_size-1]\n",
    "            chunks.append(chunk_with_md)\n",
    "\n",
    "    # work on usefull sections\n",
    "    for s in article[\"sections\"]:\n",
    "        if (s[\"title\"].lower() in sections_to_keep):\n",
    "            for p, paragraph in enumerate(s[\"paragraphs\"]):\n",
    "                for i in range(0, len(paragraph[\"text\"]), chunk_size):\n",
    "                    # print(f\"{i} : {(i)} | {i + chunk_size -1}\")\n",
    "                    chunk_with_md = md.copy()\n",
    "                    chunk_with_md[\"section\"] = s[\"title\"]\n",
    "                    chunk_with_md[\"paragraph_nb\"] = p\n",
    "                    chunk_with_md[\"chunk_nb\"] = i/chunk_size\n",
    "                    chunk_with_md[\"text\"] = paragraph[\"text\"][i:i+chunk_size-1]\n",
    "                    chunks.append(chunk_with_md)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_covariates(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    # Extract covariates using list comprehension\n",
    "    covariates = [line.split(': ')[1].strip().rstrip('.') for line in lines if 'Covariate found:' in line or 'No covariate found:' in line]\n",
    "    # Join the covariates into a single string\n",
    "    return [', '.join(covariates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disease in list_of_diseases:\n",
    "    tei_files_path_disease = tei_files_path + \"/\" + disease\n",
    "\n",
    "    list_of_chunk = []\n",
    "    for doc in os.listdir(tei_files_path_disease):\n",
    "        list_of_chunk.extend(chunk_relevant_sections( os.path.join(tei_files_path_disease, doc)))\n",
    "    df = pd.DataFrame(list_of_chunk)\n",
    "\n",
    "    for pretrained_model in list_of_base_models:\n",
    "        print(f\"Work on: {pretrained_model}\")\n",
    "        results_list_infer_gpt3_5 = []\n",
    "\n",
    "        for sentence in df[\"text\"]:\n",
    "        # for sentence in df.iloc[0:3][\"text\"]:\n",
    "            if pretrained_model == 'GPT-3.5':\n",
    "                res = rag_chain.invoke(\"Is there any covariate (or risk factor) in this following sentence: \\n \" + sentence)\n",
    "            else:\n",
    "                res = rag_chain_gpt4.invoke(\"Is there any covariate (or risk factor) in this following sentence: \\n \" + sentence + \"please provide only a python list like '[temperature, humidity] without any explanation'\")\n",
    "            results_list_infer_gpt3_5.append(res)\n",
    "        \n",
    "        df[pretrained_model] = pd.DataFrame(results_list_infer_gpt3_5)\n",
    "        if pretrained_model == 'GPT-3.5':#need to parse because GPT-3.5 doest not follow well the instructions\n",
    "            df[pretrained_model] = df[pretrained_model].apply(extract_covariates)\n",
    "    \n",
    "    df.to_csv(f\"./data/whole_inference_llm_{disease}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[pretrained_model] = df[pretrained_model].apply(extract_covariates)\n",
    "# df.to_csv(f\"./data/whole_inference_llm_{disease}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_luca_review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
