{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate NER annotation from the 2nd annotation campaign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/quantitative_value_with_context_CC.xlsx'\n",
    "\n",
    "# Read the \"missing value\" worksheet\n",
    "missing_value_df = pd.read_excel(file_path, sheet_name='missing value')\n",
    "\n",
    "# Read the 'quantitative_value_with context' worksheet\n",
    "quantitative_value_df = pd.read_excel(file_path, sheet_name='quantitative_value_with context')\n",
    "# quantitative_value_df = quantitative_value_df.rename(columns={'Relevant context for MOOD data extraction': 'context'})\n",
    "\n",
    "missing_value_df['source'] = 'missing_value'\n",
    "quantitative_value_df['source'] = 'quantitative_value_with_context'\n",
    "\n",
    "# drop 3 lines corresponding to a Claudia's comment\n",
    "\"\"\"\n",
    "Missing values not found by the script. Articles: MB7, MB8, CC6-15-17-23-32\n",
    "\"\"\"\n",
    "missing_value_df = missing_value_df.drop(index=range(53, 56))\n",
    "\n",
    "# concatenate the 2 dataframe\n",
    "# df = pd.concat([missing_value_df, quantitative_value_df], ignore_index=True)\n",
    "df = quantitative_value_df\n",
    "\n",
    "usable_covariates = df[df[\"Relevant context for MOOD data extraction\"].str.lower().isin([\"yes\", \"Yes\"])]\n",
    "# usable_covariates = pd.concat([missing_value_df, usable_covariates])\n",
    "\n",
    "full_text_annotation = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"no\", \"No\"])]\n",
    "table_annotations = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"Table\", \"table\", \"table and caption\"])]\n",
    "figure_annotations = usable_covariates[usable_covariates[\"Mood extraction from Table/Figure\"].str.lower().isin([\"figure\", \"Figure\", \"Figure caption\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_covariates[\"Mood extraction from Table/Figure\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_for_training = full_text_annotation\n",
    "\n",
    "# annotation_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "token_list = []\n",
    "label_list = []\n",
    "ner_tags_list = []\n",
    "for i, row in annotation_for_training.iterrows():\n",
    "    texts = row[\"context\"]\n",
    "    labels_xls = row[\"non-standardized covariate in the context\"].split(\",\")\n",
    "    labels_xls = [x.strip(' ') for x in labels_xls]\n",
    "    # sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", texts)\n",
    "    sentence = texts\n",
    "\n",
    "    # for sentence in sentences:\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    ner_tags = []\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    for token in tokenized_sentence:\n",
    "        if token.startswith(\"##\"):  # Subword token\n",
    "            tokens[-1] += token[2:]  # Concatenate to the previous token\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    token_list.append(tokens)                   \n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    ner_tags = [0] * len(tokens)\n",
    "    for i, tokens in enumerate(tokens):\n",
    "        if any(tokens in target_token for target_token in labels_xls):\n",
    "            labels[i] = \"covariate\"\n",
    "            ner_tags[i] = 1\n",
    "    label_list.append(labels)\n",
    "    ner_tags_list.append(ner_tags)\n",
    "\n",
    "print(token_list)\n",
    "print(label_list)\n",
    "print(ner_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"tokens\": token_list, \"ner_tags\": ner_tags_list, \"labels\": label_list})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(f'Token: {dataset[i][\"tokens\"]} \\nLabel: {dataset[i][\"ner_tags\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_size = 0.4\n",
    "dataset_dict = dataset.train_test_split(test_size=splited_size, shuffle=False)\n",
    "\n",
    "dataset_dict.save_to_disk(\"./data/annotation_generated_from_xlsx/annotation.dataset\")\n",
    "dataset_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate new annotations using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import glob\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "For a Name Entity Recognition task, I want to generate other training data based on those 2 examples bellow. \n",
    "The main objectif is to find, in scientific articles, risk factors (that we call 'covariate') that impact the spreading of disease\n",
    "\n",
    "Example 1:\n",
    "sentence: [{annotation_for_training[\"context\"].iloc[0]}]\n",
    "Covariate: [{annotation_for_training[\"non-standardized covariate in the context\"].iloc[0]}]\n",
    "\n",
    "Example 2:\n",
    "sentence: [{annotation_for_training[\"context\"].iloc[3]}]\n",
    "Covariate: [{annotation_for_training[\"non-standardized covariate in the context\"].iloc[3]}]\n",
    "\n",
    "I need you to generate completly 20 news sentences in a python list format and another list which contains the exact names of covariates.\n",
    "Don't give explaination, only return python3 code containing two line: 1rst sentence_list then 2nd covariate_list, and nothing else. Don't give name to list. Don't indent\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo-0301\",  # Specify your chat model name\n",
    "  messages=[{\"role\": \"system\", \"content\": \"You are chatBot that provide 2 lists\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "  # max_tokens=200,\n",
    "  n=10  # Number of samples to generate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = [message[\"message\"][\"content\"] for message in response[\"choices\"]]\n",
    "\n",
    "list_sentences = []\n",
    "list_covariates = []\n",
    "\n",
    "for text in generated_texts:\n",
    "    try:\n",
    "        sentences = eval(text.split(\"\\n\\n\")[0])\n",
    "        covariates = eval(text.split(\"\\n\\n\")[1])\n",
    "        list_sentences.extend(sentences)\n",
    "        list_covariates.extend(covariates)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "list_ner_tags = []\n",
    "list_tokens = []\n",
    "list_labels = []\n",
    "for j, sentence in enumerate(list_sentences):\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    for token in tokenized_sentence:\n",
    "        if token.startswith(\"##\"):  # Subword token\n",
    "            tokens[-1] += token[2:]  # Concatenate to the previous token\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    list_tokens.append(tokens)                   \n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    ner_tags = [0] * len(tokens)\n",
    "    for i, tokens in enumerate(tokens):\n",
    "        try:\n",
    "            if (tokens in list_covariates[j]):\n",
    "                ner_tags[i] = 1\n",
    "                labels[i] = \"covariate\"\n",
    "        except:\n",
    "            print(f\"j: {j} | tokens: {tokens} | list_covariates: {list_covariates}\")\n",
    "    list_ner_tags.append(ner_tags)\n",
    "    list_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(list_tokens)}: {list_tokens}\")\n",
    "# print(f\"{len(list_covariates)}: {list_covariates}\")\n",
    "print(f\"{len(list_ner_tags)}: {list_ner_tags}\")\n",
    "print(f\"{len(list_labels)}: {list_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chatgpt = Dataset.from_dict({\"tokens\": list_tokens, \"ner_tags\": list_ner_tags, \"labels\": list_labels})\n",
    "dataset_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "dataset_dict[\"train\"] = concatenate_datasets([dataset_dict[\"train\"], (dataset_chatgpt)])\n",
    "dataset_dict.save_to_disk(\"./data/annotation_generated_from_xlsx/annotation_chatgpt_augmented.dataset\")\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_luca_review_avian_flu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
